{"componentChunkName":"component---src-pages-analyze-ws-ml-dev-mdx","path":"/analyze/ws-ml-dev/","result":{"pageContext":{"frontmatter":{"title":"Watson Studio - developing the predictive model","description":"Watson Studio - developing the predictive model"},"relativePagePath":"/analyze/ws-ml-dev.mdx","titleType":"append","MdxNode":{"id":"063d6aed-a078-568c-8b14-ba8b31c74b80","children":[],"parent":"9d7f4037-d759-5fae-9e59-0c1ab953353e","internal":{"content":"---\ntitle: Watson Studio - developing the predictive model\ndescription:  Watson Studio - developing the predictive model\n---\n\nCloud Pak for data integrates [Watson Studio](https://www.ibm.com/cloud/watson-studio) to develop machine learning models and do feature engineering. In this chapter we use [AutoAI](https://www.ibm.com/cloud/watson-studio/autoai) to develop the model. In the following chapter [Developing the Anomaly Detection Model with Python and Custom Notebook in Watson Studio](../ws-ml-py-dev.mdx) we create the model using a notebook.\n\nThe Data Steward has prepared a dataset by joining different datasources, but he did not pay attention to the column semantic for building a machine learning. So the Data Scientist will start to review and adapt the data.\n\n## Data analysis and refinery\n\nThe data scientist wants to do at least two things on the current data set: remove unnecessary features (the longitude and lattitude will not have any value to assess the sensor anomaly), and transform the problem to be a classification problem by adding a label column. \n\nNote that we could have model the problem using unsupervised learning and identify anomaly with clustering or anomaly detection. This will be done in the future to present a more realistic approach to this classical industrial problem.\n\nFor that, we use the `Data Refinery` capability of Cloud Pak for Data:\n\n![Access to Refinery](images/access-refine.png)\n\nand then clean and shape the data to prepare for the model. For example remove columns like latitude, longitude, timestamp, _id, telemetry_id:\n\n![Remove columns](images/remove-column.png)\n\nTo define a new column to be used as label, we use the `Operation` tab to add `Conditional replace` to apply some basic logic on the sensor columns by using one thresholds: test the value of the CO2 to be greater than or equal to threshold (22):\n\n![Define condition](images/a_anomally_def_1.png)\n\n\nWe add the new label (Issue =1, NoIssue=0) in the Issue Flag column:\n\nWhich translates to something like:\n\"Replaced values for Issue: carbon_dioxide_level where value is greater than or equal to 22 as 1. Replaced all remaining values with 0.\"\n\n\nOnce the label column is added, and any new derived data are added, we can start a `refinery job`, that will create a new dataset in our project:\n\n![Refinery job](images/a_dr_job_1.png)\n\n\n## AutoAI\n\nAutoAI uses data to automatically select the best supervised algorithm to determine the best classification or regression models with optimized hyper parameters.\n\nWithin a project, we can add an `Auto AI Experiment`:\n\n![Auto AI Experiment](images/autoai-experiment.png)\n\nThen specify a name and server configuration:\n\n![Auto AI Experiment name](images/autoai-experiment-2.png)\n\nAdd a existing data source (the one prepared by the refinery job), and then specify the column to use for prediction (Issue Flag column):\n\n![](images/a_autoai_start_1.png)\n\nThen execute the prediction experiment.\n\nAutoAI will do different steps, split the data, prepare data, and then select model or algorithm that may better address the problem. For classification model, it will select among 30 potential candidates: decision tree, random forest, LGBM, XGBoost... Each algorithm selection will generate a pipeline which will be scored to present the most accurate one:\n\n![autoai](images/autoai-1.png)\n\n\nEach model pipeline is scored for a variety of metrics and then ranked. The default ranking metric for binary classification models is the area under the ROC curve, for multi-class classification models is accuracy, and for for regression models is the root mean-squared error (RMSE). The highest-ranked pipelines are displayed in a leaderboard, so you can view more information about them. The leaderboard also provides the option to save select model pipelines after reviewing them.\n\n![autoai](images/a_Autoai_2.png)\n\nThe resulting experiments are ranked.\n\n![autoai](images/autoai-2.png)\n\nWhen models or pipelines are created, we can see the details of each model, to see how they performed.\n\n![model-evaluation](images/a_model_evaluation_1.png)\n\n\nThe confusion matrix for the experiment ranked #1:\n\n\n![confusion-matrix](images/a_Confution_Matrix_1.png)\n\n\nThe tool offers nice capabilities like the feature transformation:\n\n![feature-transform](images/a_Feature_Transformation_1.png)\n\nAnd the feature importance ranking, which helps to assess what are the features that impact the classification.\n\n![feature-importance](images/a_Feature_Importance_1.png)\n\nOnce the model is saved, it can be added to a Catalog, and then being deployed.\n\n![saved-models](images/a_model_publishcatalog_promote_1.png)\n\n### Model Deployment\n\nOnce a model is created is promoted to a \"space\".\n\n![model-to-promote](images/a_promote_to_space_1.png)\n\nA **space** contains an overview of deployment status, the deployable assets, associated input and output data, and the associated environments.\n\nTo access the deployment space, use the main left menu under `Deployments`.\n\n![spaces](images/a_deployments_1.png)\n\n\nAnd under Deployments we can see the deployment spaces and activity related to those spaces:\n\n\n![space](images/a_deployment_spaces_1.png)\n\n![space](images/a_deployment_activity_1.png)\n\nOn the Assets page, you can view the assets in the deployment space (Here we have our AutoAI experimental model deployed). You can see how many deployments each asset has and whether the asset is configured for performance batch scoring or monitoring. The following figure displays the deployed model\n\n\n![service](images/a_deployment_space_assets_1.png)\n\n![service](images/a_deployment_space_deployments_1.png)\n\n![service](images/a_deployment_space_batchscoring_job_1.png)\n\n\nAnd going into the deployed model view, we can see the API end point:\n\n![api](images/a_model_endpoint.png)\n\nAnd even test the prediction from the user interface.\n\n![test](images/a_online_model_test_1.png)\n\nA next step is to infuse this model into the scoring application...\n\n\n\n## Configuring Model in OpenScale\n\nThis step involves configuring model in UAT for performance validation.\n\nIn this step, we shall configure various Monitors for the model, namely Fairness, Drift, Accuracy.\nAlso using Watson OpenScale one can explain each prediction by indicating the relative importance of the features in arriving at the prediction.\n\n\n*Note: UAT and Pre-prod can be used interchangeably*.\n\nGo to Instances \n\n![test](images/a_instances_1.png)\n\nopen the openscale dashboard\n\n![test](images/a_instances_openscale_1.png)\n\nGo to the OpenScale Dashboard, there you would see Add to Dashboard/ Add button that can be used to add a specific deployment for monitoring purpose. This will bring up a popup window showing list of available deployments. This list must show the deployment(s) you have created in previous steps.\n\nSelet the appropriate deployment and press Configure and it will bring you to the next screen that would show you a dialog box to Configure Monitors. Click on the Configure Monitor and that would take you to the screen below.\n\n![test](images/a_model_deployment_1.png)\n\n\n![test](images/a_model_deployment_2.png)\n\n\nConfigure the Model info and Evalualtion details. Now it's ready for Model monitoring in UAT and you also can evaluate model performance from Openscale.\n\n\n## Pre-Prod Model Evaluation & Approval\n\n\nThis sub-step involves evaluating the models deployed in pre-prod/UAT environment using Openscale and comparing different models to decide the optimal model to be moved to production in later steps.\n\nPre-prod Model Evaluation\n\nAfter you are done with configuring the model parameters, you have to evaluate the Model (by going through that Actions drop down) by passing the Feedback data.\n\n![test](images/a_model_deployment_3.png)\n\nClick on Evaluate Now, select Import from CSV file and browse for the customer_churn_quality_feedback file on your system. This is made available in the data assets which you can download into your system.\n\n![test](images/a_model_deployment_4.png)\n\nOnce you have uploaded the feedback file and clicked on Upload and Evaluate, it may take some time to generate the performance statistics (Fairness/Quality/Drift)\n\n![test](images/a_model_deployment_5.png)\n\nModel Approval\n\nOnce you're satisfied with the deployed model and its performance, approve the model for production (again going through the link in the Actions drop down menu).\n\n![test](images/a_model_deployment_9.png)\n\n\nYou may also download the Report PDF showcasing the statistics of the chosen model and its associated summary in concise format in the pdf.\n\n![test](images/a_model_deployment_6.png)\n\n![test](images/a_model_deployment_7.png)\n\n![test](images/a_model_deployment_8.png)\n\n## Model Deployment and Configuration for Monitoring in Production Environment\n\nOnce a model is validated in UAT, deploy the model in a production environment by following the steps below.\n\nThis step involves exporting the desired model which was deployed in UAT environment and was later approved to move from from UAT environment to production. \n\n\nExport UAT Deployment space\n\n  Select Vaidation Model Validation Space in Deployments (Menu->Deployments-> Spaces).\n\n  Create an export by clicking Export Space icon shown in the image below.\n\n\n![test](images/a_model_deployment_10.png)\n\n\nClick on New Export File. Select the Models/asset you intend to Export. Give an appropriate file name and click on Create.\n\nOnce the export file has been created, download the zip file on your system. \n\n\n\nCreate Production Deployment Space\n\nCreate a prod deployment space by importing the file you saved in the previous step.\n\nGo to Deployments on left pane in Cloud Pak (shown previously). Click on New deployment space and select Create a space from a file.\n\nNow upload the file into this deployment space and click on Create with an appropriate name for the deployment space.\n\nAfter that is done, you can go to the newly created deployment space and see the models you created in previous step and was moved to production after inspection. \n\n![test](images/a_model_deployment_11.png)\n\nNow add admin as a collaborator using the access control tab. This is required for OpenScale model monitoring.\n\n\n\nProd Model Deployment\n\nThe Model will appear in the Assets tab (that you had approved last time in UAT Deployment Space). Deploy the Model (select online deployment type) and give the deployment a unique name appending to Prod_Deployment.\n\nThe Deployed model will appear in the Deployments tab under the same deployment Space. \n\n\nConfiguring Prod Model in OpenScale\n\n\nGo to OpenScale and add a Machine learning provider\n\n   Select Watson Machine Learning (V2) as the Service provider type\n   Select the production space you just created in the dropdown of Select space\n\n   Select Environment type as Production.\n\nAdd Model for Monitoring\n\n   Now go back to the OpenScale dashboard. Click on Add to Dashboard or Add. It will show a list of deployments in a popup window.\n   Select your prod deployment and press Configure.\n   Click on Configure and Press Configure monitors in the next dialog box.\n   Click on Import settings\n   \n   \n\n  Please wait for sometime. It imports the settings of the model you had configured for UAT/ pro-prod environment.\n\n  When the import is done, all the parameters (Fairness/Quality/Model parameters etc.) and configuration/settings would automatically be imported.\n\n\n\n","type":"Mdx","contentDigest":"65aeae7816c9aef1811a71c0b362b289","counter":311,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Watson Studio - developing the predictive model","description":"Watson Studio - developing the predictive model"},"exports":{},"rawBody":"---\ntitle: Watson Studio - developing the predictive model\ndescription:  Watson Studio - developing the predictive model\n---\n\nCloud Pak for data integrates [Watson Studio](https://www.ibm.com/cloud/watson-studio) to develop machine learning models and do feature engineering. In this chapter we use [AutoAI](https://www.ibm.com/cloud/watson-studio/autoai) to develop the model. In the following chapter [Developing the Anomaly Detection Model with Python and Custom Notebook in Watson Studio](../ws-ml-py-dev.mdx) we create the model using a notebook.\n\nThe Data Steward has prepared a dataset by joining different datasources, but he did not pay attention to the column semantic for building a machine learning. So the Data Scientist will start to review and adapt the data.\n\n## Data analysis and refinery\n\nThe data scientist wants to do at least two things on the current data set: remove unnecessary features (the longitude and lattitude will not have any value to assess the sensor anomaly), and transform the problem to be a classification problem by adding a label column. \n\nNote that we could have model the problem using unsupervised learning and identify anomaly with clustering or anomaly detection. This will be done in the future to present a more realistic approach to this classical industrial problem.\n\nFor that, we use the `Data Refinery` capability of Cloud Pak for Data:\n\n![Access to Refinery](images/access-refine.png)\n\nand then clean and shape the data to prepare for the model. For example remove columns like latitude, longitude, timestamp, _id, telemetry_id:\n\n![Remove columns](images/remove-column.png)\n\nTo define a new column to be used as label, we use the `Operation` tab to add `Conditional replace` to apply some basic logic on the sensor columns by using one thresholds: test the value of the CO2 to be greater than or equal to threshold (22):\n\n![Define condition](images/a_anomally_def_1.png)\n\n\nWe add the new label (Issue =1, NoIssue=0) in the Issue Flag column:\n\nWhich translates to something like:\n\"Replaced values for Issue: carbon_dioxide_level where value is greater than or equal to 22 as 1. Replaced all remaining values with 0.\"\n\n\nOnce the label column is added, and any new derived data are added, we can start a `refinery job`, that will create a new dataset in our project:\n\n![Refinery job](images/a_dr_job_1.png)\n\n\n## AutoAI\n\nAutoAI uses data to automatically select the best supervised algorithm to determine the best classification or regression models with optimized hyper parameters.\n\nWithin a project, we can add an `Auto AI Experiment`:\n\n![Auto AI Experiment](images/autoai-experiment.png)\n\nThen specify a name and server configuration:\n\n![Auto AI Experiment name](images/autoai-experiment-2.png)\n\nAdd a existing data source (the one prepared by the refinery job), and then specify the column to use for prediction (Issue Flag column):\n\n![](images/a_autoai_start_1.png)\n\nThen execute the prediction experiment.\n\nAutoAI will do different steps, split the data, prepare data, and then select model or algorithm that may better address the problem. For classification model, it will select among 30 potential candidates: decision tree, random forest, LGBM, XGBoost... Each algorithm selection will generate a pipeline which will be scored to present the most accurate one:\n\n![autoai](images/autoai-1.png)\n\n\nEach model pipeline is scored for a variety of metrics and then ranked. The default ranking metric for binary classification models is the area under the ROC curve, for multi-class classification models is accuracy, and for for regression models is the root mean-squared error (RMSE). The highest-ranked pipelines are displayed in a leaderboard, so you can view more information about them. The leaderboard also provides the option to save select model pipelines after reviewing them.\n\n![autoai](images/a_Autoai_2.png)\n\nThe resulting experiments are ranked.\n\n![autoai](images/autoai-2.png)\n\nWhen models or pipelines are created, we can see the details of each model, to see how they performed.\n\n![model-evaluation](images/a_model_evaluation_1.png)\n\n\nThe confusion matrix for the experiment ranked #1:\n\n\n![confusion-matrix](images/a_Confution_Matrix_1.png)\n\n\nThe tool offers nice capabilities like the feature transformation:\n\n![feature-transform](images/a_Feature_Transformation_1.png)\n\nAnd the feature importance ranking, which helps to assess what are the features that impact the classification.\n\n![feature-importance](images/a_Feature_Importance_1.png)\n\nOnce the model is saved, it can be added to a Catalog, and then being deployed.\n\n![saved-models](images/a_model_publishcatalog_promote_1.png)\n\n### Model Deployment\n\nOnce a model is created is promoted to a \"space\".\n\n![model-to-promote](images/a_promote_to_space_1.png)\n\nA **space** contains an overview of deployment status, the deployable assets, associated input and output data, and the associated environments.\n\nTo access the deployment space, use the main left menu under `Deployments`.\n\n![spaces](images/a_deployments_1.png)\n\n\nAnd under Deployments we can see the deployment spaces and activity related to those spaces:\n\n\n![space](images/a_deployment_spaces_1.png)\n\n![space](images/a_deployment_activity_1.png)\n\nOn the Assets page, you can view the assets in the deployment space (Here we have our AutoAI experimental model deployed). You can see how many deployments each asset has and whether the asset is configured for performance batch scoring or monitoring. The following figure displays the deployed model\n\n\n![service](images/a_deployment_space_assets_1.png)\n\n![service](images/a_deployment_space_deployments_1.png)\n\n![service](images/a_deployment_space_batchscoring_job_1.png)\n\n\nAnd going into the deployed model view, we can see the API end point:\n\n![api](images/a_model_endpoint.png)\n\nAnd even test the prediction from the user interface.\n\n![test](images/a_online_model_test_1.png)\n\nA next step is to infuse this model into the scoring application...\n\n\n\n## Configuring Model in OpenScale\n\nThis step involves configuring model in UAT for performance validation.\n\nIn this step, we shall configure various Monitors for the model, namely Fairness, Drift, Accuracy.\nAlso using Watson OpenScale one can explain each prediction by indicating the relative importance of the features in arriving at the prediction.\n\n\n*Note: UAT and Pre-prod can be used interchangeably*.\n\nGo to Instances \n\n![test](images/a_instances_1.png)\n\nopen the openscale dashboard\n\n![test](images/a_instances_openscale_1.png)\n\nGo to the OpenScale Dashboard, there you would see Add to Dashboard/ Add button that can be used to add a specific deployment for monitoring purpose. This will bring up a popup window showing list of available deployments. This list must show the deployment(s) you have created in previous steps.\n\nSelet the appropriate deployment and press Configure and it will bring you to the next screen that would show you a dialog box to Configure Monitors. Click on the Configure Monitor and that would take you to the screen below.\n\n![test](images/a_model_deployment_1.png)\n\n\n![test](images/a_model_deployment_2.png)\n\n\nConfigure the Model info and Evalualtion details. Now it's ready for Model monitoring in UAT and you also can evaluate model performance from Openscale.\n\n\n## Pre-Prod Model Evaluation & Approval\n\n\nThis sub-step involves evaluating the models deployed in pre-prod/UAT environment using Openscale and comparing different models to decide the optimal model to be moved to production in later steps.\n\nPre-prod Model Evaluation\n\nAfter you are done with configuring the model parameters, you have to evaluate the Model (by going through that Actions drop down) by passing the Feedback data.\n\n![test](images/a_model_deployment_3.png)\n\nClick on Evaluate Now, select Import from CSV file and browse for the customer_churn_quality_feedback file on your system. This is made available in the data assets which you can download into your system.\n\n![test](images/a_model_deployment_4.png)\n\nOnce you have uploaded the feedback file and clicked on Upload and Evaluate, it may take some time to generate the performance statistics (Fairness/Quality/Drift)\n\n![test](images/a_model_deployment_5.png)\n\nModel Approval\n\nOnce you're satisfied with the deployed model and its performance, approve the model for production (again going through the link in the Actions drop down menu).\n\n![test](images/a_model_deployment_9.png)\n\n\nYou may also download the Report PDF showcasing the statistics of the chosen model and its associated summary in concise format in the pdf.\n\n![test](images/a_model_deployment_6.png)\n\n![test](images/a_model_deployment_7.png)\n\n![test](images/a_model_deployment_8.png)\n\n## Model Deployment and Configuration for Monitoring in Production Environment\n\nOnce a model is validated in UAT, deploy the model in a production environment by following the steps below.\n\nThis step involves exporting the desired model which was deployed in UAT environment and was later approved to move from from UAT environment to production. \n\n\nExport UAT Deployment space\n\n  Select Vaidation Model Validation Space in Deployments (Menu->Deployments-> Spaces).\n\n  Create an export by clicking Export Space icon shown in the image below.\n\n\n![test](images/a_model_deployment_10.png)\n\n\nClick on New Export File. Select the Models/asset you intend to Export. Give an appropriate file name and click on Create.\n\nOnce the export file has been created, download the zip file on your system. \n\n\n\nCreate Production Deployment Space\n\nCreate a prod deployment space by importing the file you saved in the previous step.\n\nGo to Deployments on left pane in Cloud Pak (shown previously). Click on New deployment space and select Create a space from a file.\n\nNow upload the file into this deployment space and click on Create with an appropriate name for the deployment space.\n\nAfter that is done, you can go to the newly created deployment space and see the models you created in previous step and was moved to production after inspection. \n\n![test](images/a_model_deployment_11.png)\n\nNow add admin as a collaborator using the access control tab. This is required for OpenScale model monitoring.\n\n\n\nProd Model Deployment\n\nThe Model will appear in the Assets tab (that you had approved last time in UAT Deployment Space). Deploy the Model (select online deployment type) and give the deployment a unique name appending to Prod_Deployment.\n\nThe Deployed model will appear in the Deployments tab under the same deployment Space. \n\n\nConfiguring Prod Model in OpenScale\n\n\nGo to OpenScale and add a Machine learning provider\n\n   Select Watson Machine Learning (V2) as the Service provider type\n   Select the production space you just created in the dropdown of Select space\n\n   Select Environment type as Production.\n\nAdd Model for Monitoring\n\n   Now go back to the OpenScale dashboard. Click on Add to Dashboard or Add. It will show a list of deployments in a popup window.\n   Select your prod deployment and press Configure.\n   Click on Configure and Press Configure monitors in the next dialog box.\n   Click on Import settings\n   \n   \n\n  Please wait for sometime. It imports the settings of the model you had configured for UAT/ pro-prod environment.\n\n  When the import is done, all the parameters (Fairness/Quality/Model parameters etc.) and configuration/settings would automatically be imported.\n\n\n\n","fileAbsolutePath":"/home/runner/work/vaccine-solution-main/vaccine-solution-main/docs/src/pages/analyze/ws-ml-dev.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}