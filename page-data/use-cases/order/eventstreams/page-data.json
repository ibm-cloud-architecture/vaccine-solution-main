{"componentChunkName":"component---src-pages-use-cases-order-eventstreams-mdx","path":"/use-cases/order/eventstreams/","result":{"pageContext":{"frontmatter":{"title":"Order management and optimization deployed with EventStreams","description":"Order management and optimization demonstration"},"relativePagePath":"/use-cases/order/eventstreams.mdx","titleType":"append","MdxNode":{"id":"43ee9a23-79be-51d3-a052-5f705df089f5","children":[],"parent":"5ff4d543-bd95-50b6-b10e-97091de2dda4","internal":{"content":"---\ntitle: Order management and optimization deployed with EventStreams\ndescription: Order management and optimization demonstration\n--- \n\n1. Create the following artifacts in the `eventstreams` namespace on your OpenShift cluster:\n\n   1. Create an EventStreams instance _(via the [Event Streams Custom Resource](https://ibm.github.io/event-streams/installing/installing/#install-an-event-streams-instance))_.\n   2. Create a [Kafka User with SCRAM-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Reefer Simulator](https://github.com/ibm-cloud-architecture/vaccine-reefer-simulator#application-deployment).\n   3. Create a [Kafka User with TLS-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Monitoring Agent](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent#create-a-tls-user).\n   4. Create two [Kafka Topics](https://ibm.github.io/event-streams/getting-started/creating-topics/). This tutorial will assume the names of `vaccine.shipment.plans` respectively.\n\n\n## Deploy Postgresql\n\n\n```shell\ngit clone https://github.com/ibm-cloud-architecture/vaccine-gitops.git\ncd vaccine-gitops/environments\noc apply -k \n```\n\n## Deploy the Vaccine Order Service with Event Streams\n\nThis microservice is built using maven and Quarkus extensions. In this current main project we have \nWe have already pushed the last version of this service on dockerhub, if you do not want to build it. \n\n1. Ensure you are working inside the correct project via the following `oc` command:\n\n  ```shell\n  export PROJECT_NAME=vaccine-solution\n  oc project ${PROJECT_NAME}\n  ```\n\n1. Export the value of your Event Streams cluster name into an environment variable:\n\n  ```shell\n  export CLUSTER_NAME=eda-dev\n  export EVENTSTREAMS_NS=eventstreams\n  ```\n   * To check what the name of your Event Streams cluster name is do:\n   ```shell\n   $ oc get eventstreams -n ${EVENTSTREAMS_NS}\n   NAME           STATUS\n   eda-dev    Ready\n   ```\n\n1. Deploy a postgres server. The orders are persisted in an external Postgres instance running on Openshift cluster. To do a simple deployment performs the following commands:\n\n  ```shell\n   oc apply -f event-streams/infrastructure/service-account.yaml\n   oc adm policy add-scc-to-user anyuid -z vaccine-runtime -n vaccine-solution\n   oc apply -k event-streams/infrastructure/postgres\n\n  ```\n\n1. Get Kafka TLS user name to access Event Streams bootstrap using the external route. export this user name in KAFKA_USER environment variable:\n\n  ```shell\n  oc get kafkausers -n $EVENTSTREAMS_NS \n  # NAME                                CLUSTER   AUTHENTICATION   AUTHORIZATION\n  # app-scram                           eda-dev   scram-sha-512    simple\n  # app-tls                             eda-dev   tls              simple\n  export KAFKA_USER=app-tls\n  ```\n1. Copy Kafka TLS user certificate to target project:\n\n  ```shell\n  oc get secret ${KAFKA_USER} -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.namespace=\"'${PROJECT_NAME}'\"' | oc apply -f -\n  ```\n1. Get Kafka bootstrap server URL within\n\n   ```shell\n   export KAFKA_BROKERS=$(oc get route -n ${EVENTSTREAMS_NS} ${CLUSTER_NAME}-kafka-bootstrap -o jsonpath=\"{.status.ingress[0].host}:443\")\n   ```\n\n 1. Copy TLS server CA certificate from eventstreams project to local project with the command:\n\n   ```shell\n   oc get secret ${CLUSTER_NAME}-cluster-ca-cert -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.name=\"kafka-cluster-ca-cert\"' |jq -r '.metadata.namespace=\"'${PROJECT_NAME}'\"' | oc apply -f -\n   ``` \n1. Deploy the order management microservice\n\n  ```shell\n  oc apply -f ./apps/order-mgt/base/order-mgt-configmap.yaml\n  oc apply -f ./apps/order-mgt/base/order-mgt-deployconfig.yaml\n  ```\n\n### Deploy Debezium CDC connector\n\nThe [Event Streams product documentation](https://ibm.github.io/event-streams/connecting/setting-up-connectors/) goes over the tasks to be done to config Kafka Connect cluster, but we can summarize them for our use case as:\n\n* Start a Kafka connector cluster: We use the custom resource definition called `KafkaConnectS2I`, which one instance represents a Kafka connect cluster. Each connector is represented by another custom resource called `KafkaConnector`. Kafka connect needs a user access to the Brokers. We can use the TLS user as previously done (See also [this note](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/connect-cos/#set-up-the-kafka-connect-cluster) for the same type of setting). \n  * Event Streams UI has a Toolbox menu with the `Set up a Kafka Connect environment` where we can download the `KafkaConnectS2I` configuration. The matching configuration is [in this file](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/main/environment/cdc/kafka-connect-s2i.yaml) and uses predefined TLS user and cluster certificate. The cluster name is `connect-cluster`.\n  * Update the following values in this file: `bootstrapServers` and `secretName: tls`  to `secretName: <yourTLSuser>` and the Server ca certificate secretName like `kafka-cluster-ca-cert`.\n   ```yaml\n   tls:\n    trustedCertificates:\n      - certificate: ca.crt\n        secretName: kafka-cluster-ca-cert\n   ```\n  * Deploy the cluster with: `oc apply -f kafka-connect-s2i.yaml -n ${EVENTSTREAMS_NS}`\n  * Validate it via: \n\n   ```shell\n   oc get kafkaconnects2i -n ${EVENTSTREAMS_NS}\n   oc describe kafkaconnects2i connect-cluster -n ${EVENTSTREAMS_NS}\n   ```\n* Download the postgres plugin archive from [debezium maven repository](https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.4.0.Final/debezium-connector-postgres-1.4.0.Final-plugin.tar.gz) and then add the jar files to the `my-plugins\\debezium-connector` folder. We need a subfolder as this connector includes multiple jars. This step was already done and the debezium connector jars are in the [environment/cdc/my-plugins/debezium-connector)](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/tree/main/environment/cdc/my-plugins/debezium-connector)\n\n  ```\n  ├── my-plugins\n  │   └── debezium-connector\n  │       ├── CHANGELOG.md\n  │       ├── CONTRIBUTE.md\n  │       ├── COPYRIGHT.txt\n  │       ├── LICENSE-3rd-PARTIES.txt\n  │       ├── LICENSE.txt\n  │       ├── README.md\n  │       ├── README_ZH.md\n  │       ├── debezium-api-1.4.0.Final.jar\n  │       ├── debezium-connector-postgres-1.4.0.Final.jar\n  │       ├── debezium-core-1.4.0.Final.jar\n  │       ├── failureaccess-1.0.1.jar\n  │       ├── guava-30.0-jre.jar\n  │       ├── postgresql-42.2.14.jar\n  │       └── protobuf-java-3.8.0.jar\n  └── pg-connector.yaml\n  ```\n\n* Deploy the connector configuration:\n\n  ```shell\n  # pwd = .../environment/cdc/\n  oc start-build connect-cluster-connect --from-dir ./my-plugins/ --follow -n ${EVENTSTREAMS_NS}\n  #...\n  # Storing signatures\n  # Successfully pushed image-registry.openshift-image-registry.svc:5000/eventstreams/connect-cluster-connect@sha256:9315b6a6c8f904d0fb5a57f67ba4c9067c6c8264814f283151b20b9d6f147092\n  # Push successful\n  ```\n* Modify the `pg-connector.yaml` from the `environment/cdc` folder to configure the Postgres datasource credentials:\n  ```yaml\n  config: \n    database.dbname: postgres\n    database.hostname: postgres.vaccineorder.svc\n    database.password: pgpwd\n    database.port: 5432\n    database.server.name: vaccine\n    database.user: postgres\n    table.whitelist: public.orderevents\n    plugin.name: pgoutput\n  ```\n* Then start the connector: `oc apply -f pg-connector.yaml -n ${EVENTSTREAMS_NS}`\n* Verify it is running: `oc describe kafkaconnector pg-connector -n ${EVENTSTREAMS_NS}`, you should see one task running. \n* Looking at the pod trace for the connector you should see a successful connection, something like:\n\n```\nSuccessfully tested connection for jdbc:postgresql://postgres.vaccineorder.svc:5432/postgres with user 'postgres' \n```\n* A new topic may have been created with the name of the table replicated: `vaccine.public.orderevents` with new messages mapping the rows in the table.","type":"Mdx","contentDigest":"e0272610755f5307c5b03eca1b20c3f3","counter":329,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Order management and optimization deployed with EventStreams","description":"Order management and optimization demonstration"},"exports":{},"rawBody":"---\ntitle: Order management and optimization deployed with EventStreams\ndescription: Order management and optimization demonstration\n--- \n\n1. Create the following artifacts in the `eventstreams` namespace on your OpenShift cluster:\n\n   1. Create an EventStreams instance _(via the [Event Streams Custom Resource](https://ibm.github.io/event-streams/installing/installing/#install-an-event-streams-instance))_.\n   2. Create a [Kafka User with SCRAM-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Reefer Simulator](https://github.com/ibm-cloud-architecture/vaccine-reefer-simulator#application-deployment).\n   3. Create a [Kafka User with TLS-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Monitoring Agent](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent#create-a-tls-user).\n   4. Create two [Kafka Topics](https://ibm.github.io/event-streams/getting-started/creating-topics/). This tutorial will assume the names of `vaccine.shipment.plans` respectively.\n\n\n## Deploy Postgresql\n\n\n```shell\ngit clone https://github.com/ibm-cloud-architecture/vaccine-gitops.git\ncd vaccine-gitops/environments\noc apply -k \n```\n\n## Deploy the Vaccine Order Service with Event Streams\n\nThis microservice is built using maven and Quarkus extensions. In this current main project we have \nWe have already pushed the last version of this service on dockerhub, if you do not want to build it. \n\n1. Ensure you are working inside the correct project via the following `oc` command:\n\n  ```shell\n  export PROJECT_NAME=vaccine-solution\n  oc project ${PROJECT_NAME}\n  ```\n\n1. Export the value of your Event Streams cluster name into an environment variable:\n\n  ```shell\n  export CLUSTER_NAME=eda-dev\n  export EVENTSTREAMS_NS=eventstreams\n  ```\n   * To check what the name of your Event Streams cluster name is do:\n   ```shell\n   $ oc get eventstreams -n ${EVENTSTREAMS_NS}\n   NAME           STATUS\n   eda-dev    Ready\n   ```\n\n1. Deploy a postgres server. The orders are persisted in an external Postgres instance running on Openshift cluster. To do a simple deployment performs the following commands:\n\n  ```shell\n   oc apply -f event-streams/infrastructure/service-account.yaml\n   oc adm policy add-scc-to-user anyuid -z vaccine-runtime -n vaccine-solution\n   oc apply -k event-streams/infrastructure/postgres\n\n  ```\n\n1. Get Kafka TLS user name to access Event Streams bootstrap using the external route. export this user name in KAFKA_USER environment variable:\n\n  ```shell\n  oc get kafkausers -n $EVENTSTREAMS_NS \n  # NAME                                CLUSTER   AUTHENTICATION   AUTHORIZATION\n  # app-scram                           eda-dev   scram-sha-512    simple\n  # app-tls                             eda-dev   tls              simple\n  export KAFKA_USER=app-tls\n  ```\n1. Copy Kafka TLS user certificate to target project:\n\n  ```shell\n  oc get secret ${KAFKA_USER} -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.namespace=\"'${PROJECT_NAME}'\"' | oc apply -f -\n  ```\n1. Get Kafka bootstrap server URL within\n\n   ```shell\n   export KAFKA_BROKERS=$(oc get route -n ${EVENTSTREAMS_NS} ${CLUSTER_NAME}-kafka-bootstrap -o jsonpath=\"{.status.ingress[0].host}:443\")\n   ```\n\n 1. Copy TLS server CA certificate from eventstreams project to local project with the command:\n\n   ```shell\n   oc get secret ${CLUSTER_NAME}-cluster-ca-cert -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.name=\"kafka-cluster-ca-cert\"' |jq -r '.metadata.namespace=\"'${PROJECT_NAME}'\"' | oc apply -f -\n   ``` \n1. Deploy the order management microservice\n\n  ```shell\n  oc apply -f ./apps/order-mgt/base/order-mgt-configmap.yaml\n  oc apply -f ./apps/order-mgt/base/order-mgt-deployconfig.yaml\n  ```\n\n### Deploy Debezium CDC connector\n\nThe [Event Streams product documentation](https://ibm.github.io/event-streams/connecting/setting-up-connectors/) goes over the tasks to be done to config Kafka Connect cluster, but we can summarize them for our use case as:\n\n* Start a Kafka connector cluster: We use the custom resource definition called `KafkaConnectS2I`, which one instance represents a Kafka connect cluster. Each connector is represented by another custom resource called `KafkaConnector`. Kafka connect needs a user access to the Brokers. We can use the TLS user as previously done (See also [this note](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/connect-cos/#set-up-the-kafka-connect-cluster) for the same type of setting). \n  * Event Streams UI has a Toolbox menu with the `Set up a Kafka Connect environment` where we can download the `KafkaConnectS2I` configuration. The matching configuration is [in this file](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/main/environment/cdc/kafka-connect-s2i.yaml) and uses predefined TLS user and cluster certificate. The cluster name is `connect-cluster`.\n  * Update the following values in this file: `bootstrapServers` and `secretName: tls`  to `secretName: <yourTLSuser>` and the Server ca certificate secretName like `kafka-cluster-ca-cert`.\n   ```yaml\n   tls:\n    trustedCertificates:\n      - certificate: ca.crt\n        secretName: kafka-cluster-ca-cert\n   ```\n  * Deploy the cluster with: `oc apply -f kafka-connect-s2i.yaml -n ${EVENTSTREAMS_NS}`\n  * Validate it via: \n\n   ```shell\n   oc get kafkaconnects2i -n ${EVENTSTREAMS_NS}\n   oc describe kafkaconnects2i connect-cluster -n ${EVENTSTREAMS_NS}\n   ```\n* Download the postgres plugin archive from [debezium maven repository](https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.4.0.Final/debezium-connector-postgres-1.4.0.Final-plugin.tar.gz) and then add the jar files to the `my-plugins\\debezium-connector` folder. We need a subfolder as this connector includes multiple jars. This step was already done and the debezium connector jars are in the [environment/cdc/my-plugins/debezium-connector)](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/tree/main/environment/cdc/my-plugins/debezium-connector)\n\n  ```\n  ├── my-plugins\n  │   └── debezium-connector\n  │       ├── CHANGELOG.md\n  │       ├── CONTRIBUTE.md\n  │       ├── COPYRIGHT.txt\n  │       ├── LICENSE-3rd-PARTIES.txt\n  │       ├── LICENSE.txt\n  │       ├── README.md\n  │       ├── README_ZH.md\n  │       ├── debezium-api-1.4.0.Final.jar\n  │       ├── debezium-connector-postgres-1.4.0.Final.jar\n  │       ├── debezium-core-1.4.0.Final.jar\n  │       ├── failureaccess-1.0.1.jar\n  │       ├── guava-30.0-jre.jar\n  │       ├── postgresql-42.2.14.jar\n  │       └── protobuf-java-3.8.0.jar\n  └── pg-connector.yaml\n  ```\n\n* Deploy the connector configuration:\n\n  ```shell\n  # pwd = .../environment/cdc/\n  oc start-build connect-cluster-connect --from-dir ./my-plugins/ --follow -n ${EVENTSTREAMS_NS}\n  #...\n  # Storing signatures\n  # Successfully pushed image-registry.openshift-image-registry.svc:5000/eventstreams/connect-cluster-connect@sha256:9315b6a6c8f904d0fb5a57f67ba4c9067c6c8264814f283151b20b9d6f147092\n  # Push successful\n  ```\n* Modify the `pg-connector.yaml` from the `environment/cdc` folder to configure the Postgres datasource credentials:\n  ```yaml\n  config: \n    database.dbname: postgres\n    database.hostname: postgres.vaccineorder.svc\n    database.password: pgpwd\n    database.port: 5432\n    database.server.name: vaccine\n    database.user: postgres\n    table.whitelist: public.orderevents\n    plugin.name: pgoutput\n  ```\n* Then start the connector: `oc apply -f pg-connector.yaml -n ${EVENTSTREAMS_NS}`\n* Verify it is running: `oc describe kafkaconnector pg-connector -n ${EVENTSTREAMS_NS}`, you should see one task running. \n* Looking at the pod trace for the connector you should see a successful connection, something like:\n\n```\nSuccessfully tested connection for jdbc:postgresql://postgres.vaccineorder.svc:5432/postgres with user 'postgres' \n```\n* A new topic may have been created with the name of the table replicated: `vaccine.public.orderevents` with new messages mapping the rows in the table.","fileAbsolutePath":"/home/runner/work/vaccine-solution-main/vaccine-solution-main/docs/src/pages/use-cases/order/eventstreams.mdx"}}}}