{"componentChunkName":"component---src-pages-solution-orderms-voro-solution-mdx","path":"/solution/orderms/voro-solution/","result":{"pageContext":{"frontmatter":{"title":"Vaccine Order and Reefer Optimization component","description":"Detail implementation for the optimization model, and integration into the solution."},"relativePagePath":"/solution/orderms/voro-solution.mdx","titleType":"append","MdxNode":{"id":"edb16e0b-3e70-51bd-ad70-39fa83b1754c","children":[],"parent":"6df3da10-d865-588b-b082-f98fc582a8b5","internal":{"content":"---\ntitle: Vaccine Order and Reefer Optimization component\ndescription: Detail implementation for the optimization model, and integration into the solution.\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Build</AnchorLink>\n  <AnchorLink>Run locally</AnchorLink>\n  <AnchorLink>Deploy to OpenShift</AnchorLink>\n  <AnchorLink>Usage Details</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nThis service is exposing a send order and optimize API to build a shipment plan for all the vaccine lots given a new order. The implementation use CPLEX and an event driven solution to get a continuous streams of information coming frm the manufacturing, the refrigerator inventory and the available transportation capacity and cost.\n\nThe following diagram illustrates all the components working together to support and event-driven shipment plan real time optimization:\n\n![Components](./images/voro-components.png)\n\n1. The app is done in python with Flask and Gunicorn WSGI server to run in a container deployable on Openshift. The [Dockerfile is here](https://github.com/ibm-cloud-architecture/vaccine-order-optimizer/blob/master/Dockerfile)\n1. A set of [Resource classes](https://github.com/ibm-cloud-architecture/vaccine-order-optimizer/tree/master/server/api) expose REST API for orders and optimization. For demonstration purpose other resoources are implemented.\n1. The optimization component transforms the data to pandas structure for CPLEX to run. As the domain was very limited, it can run internally to the python app, but a remote client is also implemented to access the decision optimization service when the problem is becoming more complex.\n1. A set of Kafka consumers, using AVRO schema are used to get real time data.\n\n\nSee the code in this [Vaccine order optimizer github repository.](https://github.com/ibm-cloud-architecture/vaccine-order-optimizer)\n\n\nThe application is deployed on OpenShift and participates with other components to demonstrate the vaccine order processing, as illustrated below:\n\n![End to end scenario](./images/order-simulated.png)\n\n1. An external app is managing the order life cycle and can be accessible from the [Vaccine Order manager](https://github.com/ibm-cloud-architecture/vaccine-order-mgr) git repository.\n1.This Vaccine Order Reefer Optimization component.\n1. Manufacturer simulator to be send a vaccine lot inventory to the `vaccine-inventory` Kafka topic.\n1. Refrigerator manager to manage the availability of refrigerator container to carry the vaccine lots\n1. Transportation simulator to send updated itinerary update.\n\n## Build\n\nSimply build the docker image and push it to your registry. Here is an example of commands:\n\n```shell\ndocker build -t ibmcase/vaccine-order-optimizer .\n\ndocker push ibmcase/vaccine-order-optimizer\n```\n\nThe repository include a [github action workflow](https://github.com/ibm-cloud-architecture/vaccine-order-optimizer/blob/master/.github/workflows/dockerbuild.yaml) to build and push the image automatically to the [public docker registry.](https://hub.docker.com/repository/docker/ibmcase/vaccine-order-optimizer)\n\nThe flow uses a set of secrets in the git repo:\n* DOCKER_IMAGE_NAME = vaccine-order-optimizer\n* DOCKER_REPOSITORY = ibmcase\n* DOCKER_USERNAME and DOCKER_PASSWORD\n\n## Run locally\n\nIt is possible to run the code on your laptop or server but connected to Event Streams deployed on OpenShift. Some pre-requisites need to be done:\n\n* Get the Kafka URL, schema registry URL, the user and password and any pem file containing the server certificate.\n* The certificate needs to be under certs folder.\n* Copy the script/setenv-tmpl.sh  to script/setenv.sh\n* modify the environment variables.\n\n```shell\nsource ./script/setenv.sh\n\ndocker run -ti -e KAFKA_BROKERS=$KAFKA_BROKERS -e SCHEMA_REGISTRY_URL=$SCHEMA_REGISTRY_URL -e REEFER_TOPIC=$REEFER_TOPIC -e INVENTORY_TOPIC=$INVENTORY_TOPIC -e TRANSPORTATION_TOPIC=$TRANSPORTATION_TOPIC -e KAFKA_USER=$KAFKA_USER -e KAFKA_PASSWORD=$KAFKA_PASSWORD -e KAFKA_CERT=$KAFKA_CERT -p 5000:5000  ibmcase/vaccine-order-optimizer\n```\n\nThe swagger looks like:\n\n![openAPI](./images/oro-swagger.png)\n\n## Deploy to OpenShift\n\n* Connect to the vaccine project using: `oc project vaccine`\n* Modify the kubernetes/configmap.yaml with the Kafka Broker URL you are using, and if you changed the topic names too. Then do:\n\n ```shell\n oc apply -f kubernetes/configmap.yaml\n ```\n\n* Get pem certificate from eventstreams or the Kafka cluster project to the local vaccine project with a command like:\n\n ```shell\n oc get secret light-es-cluster-cert-pem  -n eventstreams --export -o yaml | oc apply -f - \n ```\n \n This pem file is mounted to the pod via the secret as:\n\n   ```yaml\n   volumeMounts:\n  - mountPath: /certs\n    name: eventstreams-cert-pem\n  volumes:\n  - name: eventstreams-cert-pem\n    secret:\n      secretName: light-es-cluster-cert-pem\n  ```\n\n and the path for the python code to access this pem file is defined in the environment variable: \n\n ```yaml\n  - name: KAFKA_CERT\n    value: /certs/es-cert.pem\n ```\n\n the name of the file is equal to the name of the {.data.es-cert.pem} field in the secret.\n \n ```yaml\n Name:         eventstreams-cert-pem\n Namespace:    vaccine\n Labels:       <none>\n Annotations:  <none>\n\n Type:  Opaque\n\n Data\n ====\n es-cert.pem:  1164 bytes\n ```\n\n* Copy the Kafka user's secret from the `eventstreams` or Kafka project to the current vaccine project. This secret has two data fields: username and password\n\n ```shell\n oc get secret app-user-tls  -n eventstreams --export -o yaml | oc apply -f - \n ```\n\nThey are used in the Deployment configuration as:\n\n ```yaml\n - name: KAFKA_USER\n    valueFrom:\n      secretKeyRef:\n        key: username\n        name: app-user-tls\n  - name: KAFKA_PASSWORD\n    valueFrom:\n      secretKeyRef:\n        key: password\n        name: app-user-tls\n ```\n\n* Deploy the application using: `oc apply -f kubernetes/app-deployment.yaml`\n\n## Usage Details\n\nThis section addresses how to validate the app is running. \n\n* Once the application is deployed on OpenShift get the routes to the external exposed URL:\n\n ```shell\n  oc describe routes vaccine-order-optimizer \n ```\n\n* Access the swagger. As an example of URL: http://vaccine-order-optimizer-vaccine......cloud/apidocs/\n\n* Trigger the data loading with the operation:\n \n ```\n curl -X POST  http://vaccine-order-optimizer-vaccine......cloud/api/v1/optimize/loadData \"\n ```\n\n * Send a new order\n\n ```\n curl -X POST -H \"Content-Type: application/json\" http://vaccine-order-optimizer-vaccine......cloud/    --data \"@./data/order1.json\"\n ```\n","type":"Mdx","contentDigest":"3728f738b9edefda037f31750e4e5d75","counter":351,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Vaccine Order and Reefer Optimization component","description":"Detail implementation for the optimization model, and integration into the solution."},"exports":{},"rawBody":"---\ntitle: Vaccine Order and Reefer Optimization component\ndescription: Detail implementation for the optimization model, and integration into the solution.\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Build</AnchorLink>\n  <AnchorLink>Run locally</AnchorLink>\n  <AnchorLink>Deploy to OpenShift</AnchorLink>\n  <AnchorLink>Usage Details</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nThis service is exposing a send order and optimize API to build a shipment plan for all the vaccine lots given a new order. The implementation use CPLEX and an event driven solution to get a continuous streams of information coming frm the manufacturing, the refrigerator inventory and the available transportation capacity and cost.\n\nThe following diagram illustrates all the components working together to support and event-driven shipment plan real time optimization:\n\n![Components](./images/voro-components.png)\n\n1. The app is done in python with Flask and Gunicorn WSGI server to run in a container deployable on Openshift. The [Dockerfile is here](https://github.com/ibm-cloud-architecture/vaccine-order-optimizer/blob/master/Dockerfile)\n1. A set of [Resource classes](https://github.com/ibm-cloud-architecture/vaccine-order-optimizer/tree/master/server/api) expose REST API for orders and optimization. For demonstration purpose other resoources are implemented.\n1. The optimization component transforms the data to pandas structure for CPLEX to run. As the domain was very limited, it can run internally to the python app, but a remote client is also implemented to access the decision optimization service when the problem is becoming more complex.\n1. A set of Kafka consumers, using AVRO schema are used to get real time data.\n\n\nSee the code in this [Vaccine order optimizer github repository.](https://github.com/ibm-cloud-architecture/vaccine-order-optimizer)\n\n\nThe application is deployed on OpenShift and participates with other components to demonstrate the vaccine order processing, as illustrated below:\n\n![End to end scenario](./images/order-simulated.png)\n\n1. An external app is managing the order life cycle and can be accessible from the [Vaccine Order manager](https://github.com/ibm-cloud-architecture/vaccine-order-mgr) git repository.\n1.This Vaccine Order Reefer Optimization component.\n1. Manufacturer simulator to be send a vaccine lot inventory to the `vaccine-inventory` Kafka topic.\n1. Refrigerator manager to manage the availability of refrigerator container to carry the vaccine lots\n1. Transportation simulator to send updated itinerary update.\n\n## Build\n\nSimply build the docker image and push it to your registry. Here is an example of commands:\n\n```shell\ndocker build -t ibmcase/vaccine-order-optimizer .\n\ndocker push ibmcase/vaccine-order-optimizer\n```\n\nThe repository include a [github action workflow](https://github.com/ibm-cloud-architecture/vaccine-order-optimizer/blob/master/.github/workflows/dockerbuild.yaml) to build and push the image automatically to the [public docker registry.](https://hub.docker.com/repository/docker/ibmcase/vaccine-order-optimizer)\n\nThe flow uses a set of secrets in the git repo:\n* DOCKER_IMAGE_NAME = vaccine-order-optimizer\n* DOCKER_REPOSITORY = ibmcase\n* DOCKER_USERNAME and DOCKER_PASSWORD\n\n## Run locally\n\nIt is possible to run the code on your laptop or server but connected to Event Streams deployed on OpenShift. Some pre-requisites need to be done:\n\n* Get the Kafka URL, schema registry URL, the user and password and any pem file containing the server certificate.\n* The certificate needs to be under certs folder.\n* Copy the script/setenv-tmpl.sh  to script/setenv.sh\n* modify the environment variables.\n\n```shell\nsource ./script/setenv.sh\n\ndocker run -ti -e KAFKA_BROKERS=$KAFKA_BROKERS -e SCHEMA_REGISTRY_URL=$SCHEMA_REGISTRY_URL -e REEFER_TOPIC=$REEFER_TOPIC -e INVENTORY_TOPIC=$INVENTORY_TOPIC -e TRANSPORTATION_TOPIC=$TRANSPORTATION_TOPIC -e KAFKA_USER=$KAFKA_USER -e KAFKA_PASSWORD=$KAFKA_PASSWORD -e KAFKA_CERT=$KAFKA_CERT -p 5000:5000  ibmcase/vaccine-order-optimizer\n```\n\nThe swagger looks like:\n\n![openAPI](./images/oro-swagger.png)\n\n## Deploy to OpenShift\n\n* Connect to the vaccine project using: `oc project vaccine`\n* Modify the kubernetes/configmap.yaml with the Kafka Broker URL you are using, and if you changed the topic names too. Then do:\n\n ```shell\n oc apply -f kubernetes/configmap.yaml\n ```\n\n* Get pem certificate from eventstreams or the Kafka cluster project to the local vaccine project with a command like:\n\n ```shell\n oc get secret light-es-cluster-cert-pem  -n eventstreams --export -o yaml | oc apply -f - \n ```\n \n This pem file is mounted to the pod via the secret as:\n\n   ```yaml\n   volumeMounts:\n  - mountPath: /certs\n    name: eventstreams-cert-pem\n  volumes:\n  - name: eventstreams-cert-pem\n    secret:\n      secretName: light-es-cluster-cert-pem\n  ```\n\n and the path for the python code to access this pem file is defined in the environment variable: \n\n ```yaml\n  - name: KAFKA_CERT\n    value: /certs/es-cert.pem\n ```\n\n the name of the file is equal to the name of the {.data.es-cert.pem} field in the secret.\n \n ```yaml\n Name:         eventstreams-cert-pem\n Namespace:    vaccine\n Labels:       <none>\n Annotations:  <none>\n\n Type:  Opaque\n\n Data\n ====\n es-cert.pem:  1164 bytes\n ```\n\n* Copy the Kafka user's secret from the `eventstreams` or Kafka project to the current vaccine project. This secret has two data fields: username and password\n\n ```shell\n oc get secret app-user-tls  -n eventstreams --export -o yaml | oc apply -f - \n ```\n\nThey are used in the Deployment configuration as:\n\n ```yaml\n - name: KAFKA_USER\n    valueFrom:\n      secretKeyRef:\n        key: username\n        name: app-user-tls\n  - name: KAFKA_PASSWORD\n    valueFrom:\n      secretKeyRef:\n        key: password\n        name: app-user-tls\n ```\n\n* Deploy the application using: `oc apply -f kubernetes/app-deployment.yaml`\n\n## Usage Details\n\nThis section addresses how to validate the app is running. \n\n* Once the application is deployed on OpenShift get the routes to the external exposed URL:\n\n ```shell\n  oc describe routes vaccine-order-optimizer \n ```\n\n* Access the swagger. As an example of URL: http://vaccine-order-optimizer-vaccine......cloud/apidocs/\n\n* Trigger the data loading with the operation:\n \n ```\n curl -X POST  http://vaccine-order-optimizer-vaccine......cloud/api/v1/optimize/loadData \"\n ```\n\n * Send a new order\n\n ```\n curl -X POST -H \"Content-Type: application/json\" http://vaccine-order-optimizer-vaccine......cloud/    --data \"@./data/order1.json\"\n ```\n","fileAbsolutePath":"/home/runner/work/vaccine-solution-main/vaccine-solution-main/docs/src/pages/solution/orderms/voro-solution.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}