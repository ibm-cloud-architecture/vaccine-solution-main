{"componentChunkName":"component---src-pages-solution-cold-monitoring-index-mdx","path":"/solution/cold-monitoring/","result":{"pageContext":{"frontmatter":{"title":"Vaccine Cold Chain Monitoring","description":"This microservice monitoring refrigerator telemetries"},"relativePagePath":"/solution/cold-monitoring/index.mdx","titleType":"append","MdxNode":{"id":"d2550a39-c1e8-5be2-a0fd-1c562c8e3ce2","children":[],"parent":"b555e3f5-81a8-55a3-ba54-97dadd6d0307","internal":{"content":"---\ntitle: Vaccine Cold Chain Monitoring\ndescription: This microservice monitoring refrigerator telemetries\n---\n<PageDescription>\nThis microservice aims to monitor the cold-chain over time and to assess if the refrigerator is still running according to specifications.\n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Build</AnchorLink>\n   <AnchorLink>Usage Details</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nThe reefer monitoring agent gets telemetry events from the `telemetries` Kafka topic and processes them using stateful operation on time window, and then creates reefer-cold-chain-violation event to the `reefer` Kafka topic in case the temperature goes over a define threshold over a specific time period. For each received metrics it can, optionally, call an anomaly detection service to compute the risk of failure. \n\n![](./images/cold-monitoring-1.png)\n\nIn case of cold chain violation the impacted vaccine lots needs to be reported as spoiled via new records logged to the blockchain hyperledger. This is the responsability of the reefer manager microservice has it has visibility of the loaded vaccine lots for each container. Telemetry events have information on the sensors and the geolocation of the reefer.\n\n**Github repository:** [vaccine-monitoring-agent](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent)\n\n**Kafka topics consumed from:** telemetries as defined by:\n\n```yaml\napiVersion: kafka.strimzi.io/v1beta1\n  kind: KafkaTopic\n  metadata:\n    name: telemetries\n    labels:\n      strimzi.io/cluster: event-streams\n  spec:\n    partitions: 10\n    replicas: 3\n    config:\n      retention.ms: 14400000\n      segment.bytes: 1073741824\n```\n\n**Kafka topics produced to:** reefers in case of anomaly detection or cold chain violation\n\n**Events reacted to:** telemetry events like:\n\n```java\npublic class TelemetryEvent {\n\n    public String containerID;\n    public Telemetry payload;\n    public String timestamp;\n    public String type;\n```\n\nand the payload:\n\n```java\n public String container_id;\n    public String measurement_time;\n    public String product_id;\n    public double temperature;\n    public double target_temperature; \n    public double ambiant_temperature; \n    public double kilowatts; \n    public double time_door_open;\n    public int content_type; \n    public int defrost_cycle;\n    public double oxygen_level; \n    public double nitrogen_level; \n    public double humidity_level;\n    public double target_humidity_level;\n    public double carbon_dioxide_level; \n    public boolean fan_1; \n    public boolean fan_2; \n    public boolean fan_3;\n    public double latitude;\n    public double longitude;\n  \n```\n**Events produced:** reefer anomaly detected and reefer cold chain violated\n\n### Code structure\n\nThe API is supported by the [ContainerResource](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent/blob/master/src/main/java/ibm/gse/eda/vaccine/coldchainagent/api/ContainerResource.java) class which exposes interactive query on the container id.\n\nThe core of the process is a Kafka Streams topology in the class [TelemetryAssessor](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent/blob/master/src/main/java/ibm/gse/eda/vaccine/coldchainagent/domain/TelemetryAssessor.java).\n\nThe [topology](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent/blob/b591e6e338cb8e9a5a8da5dde44d299f669d2309/src/main/java/ibm/gse/eda/vaccine/coldchainagent/domain/TelemetryAssessor.java#L86-L133) processes telemetry records and build a new streams with the containerID as key, and the telemetry payload as value. Then it builds a Ktable to keep aggregate per container. The aggregate is defined in [this ReeferAggregate class](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent/blob/master/src/main/java/ibm/gse/eda/vaccine/coldchainagent/domain/ReeferAggregate.java) and aims to keep max Temperature read so far, the number of time the maximum temperature is violated.\nFinally when a container reaches the maximum number of temperature violation, a new message is sent to a 'reefer' topic for down stream processing.\n\nIf you need to learn more on Kafka streams read [this introduction](https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-streams/) and do [those labs](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/kafka-streams/) to learn more on how to program with Kafka Streams.\n\nThe nice capability of Quarkus app, is most of the work is in the [application.properties](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent/blob/master/src/main/resources/application.properties) configuration. The highlights of this configuration is the fact that once deploy to OpenShift the environment variables are defined in config map and secret:\n\n```\nquarkus.openshift.env.configmaps=agent-cm\nquarkus.openshift.env.secrets=agent-secrets\n```\n\nThe SSL certificate for the server and the user are coming from secrets:\n\n```\nquarkus.openshift.env.vars.KAFKA_CERT_PATH=/deployments/certs/server/ca.p12\nquarkus.openshift.env.mapping.KAFKA_CERT_PWD.from-secret=kafka-cluster-ca-cert\nquarkus.openshift.env.mapping.KAFKA_CERT_PWD.with-key=ca.password\nquarkus.openshift.mounts.es-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.es-cert.secret-name=kafka-cluster-ca-cert\n# TLS user\nquarkus.openshift.env.mapping.USER_CERT_PWD.from-secret=${KAFKA_USER}\nquarkus.openshift.env.mapping.USER_CERT_PWD.with-key=user.password\nquarkus.openshift.env.vars.USER_CERT_PATH=/deployments/certs/user/user.p12\nquarkus.openshift.mounts.user.path=/deployments/certs/user\nquarkus.openshift.secret-volumes.user.secret-name=${KAFKA_USER}\n```\n\nThe outgoing message sent to Kafka `reefer` topic is done via microprofile reactive messaging configuration and plugin:\n\n```\nmp.messaging.outgoing.reefers.connector=smallrye-kafka\nmp.messaging.outgoing.reefers.topic=${REEFER_TOPIC:vaccine-reefers}\nmp.messaging.outgoing.reefers.key.serializer=org.apache.kafka.common.serialization.StringSerializer\nmp.messaging.outgoing.reefers.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n```\n\nThen the last part if for Kafka Streams.\n\n\n## Build\n\nAs a quarkus application it is possible to run locally with `./mvnw quarkus:dev`. It is important to configure a `.env` file with the needed environment variables to remote connect to Kafka Cluster using a SCRAM user:\n\n```\nexport KAFKA_USER=app-scram\nexport KAFKA_PASSWORD=<>\nexport KAFKA_BOOTSTRAP_SERVERS=eda-dev-kafka-bootstrap-eventstreams.<....>.cloud:443\nexport KAFKA_SSL_TRUSTSTORE_LOCATION=${PWD}/certs/truststore.p12\nexport KAFKA_SSL_TRUSTSTORE_PASSWORD=<>\nexport TELEMETRY_TOPIC=coldchain-telemetries\nexport REEFER_TOPIC=coldchain-reefers\nexport PREDICTION_ENABLED=false\nexport EDA_LOGGING_LEVEL=INFO\nexport KAFKA_SASL_MECHANISM=SCRAM-SHA-512\n```\n\nSee the [repository readme](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent) to build and run it locally or to deploy to openshift. \n\nAlso the [cold chain monitoring use case](/use-cases/cold-chain/) presents how to deploy on openshift, but it uses the Kubernetes config and source to image capability so one command will build and deploy to OpenShift:\n\n```shell\n./mvnw clean package -Dquarkus.kubernetes.deploy=true -DskipTests\n```\n\n## Usage details\n\nThe demonstration script for this component is described in the [cold chain monitoring use case](/use-cases/cold-chain/). ","type":"Mdx","contentDigest":"1a86fe93bfb20ff76d71662ad34ee6a1","counter":283,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Vaccine Cold Chain Monitoring","description":"This microservice monitoring refrigerator telemetries"},"exports":{},"rawBody":"---\ntitle: Vaccine Cold Chain Monitoring\ndescription: This microservice monitoring refrigerator telemetries\n---\n<PageDescription>\nThis microservice aims to monitor the cold-chain over time and to assess if the refrigerator is still running according to specifications.\n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Build</AnchorLink>\n   <AnchorLink>Usage Details</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nThe reefer monitoring agent gets telemetry events from the `telemetries` Kafka topic and processes them using stateful operation on time window, and then creates reefer-cold-chain-violation event to the `reefer` Kafka topic in case the temperature goes over a define threshold over a specific time period. For each received metrics it can, optionally, call an anomaly detection service to compute the risk of failure. \n\n![](./images/cold-monitoring-1.png)\n\nIn case of cold chain violation the impacted vaccine lots needs to be reported as spoiled via new records logged to the blockchain hyperledger. This is the responsability of the reefer manager microservice has it has visibility of the loaded vaccine lots for each container. Telemetry events have information on the sensors and the geolocation of the reefer.\n\n**Github repository:** [vaccine-monitoring-agent](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent)\n\n**Kafka topics consumed from:** telemetries as defined by:\n\n```yaml\napiVersion: kafka.strimzi.io/v1beta1\n  kind: KafkaTopic\n  metadata:\n    name: telemetries\n    labels:\n      strimzi.io/cluster: event-streams\n  spec:\n    partitions: 10\n    replicas: 3\n    config:\n      retention.ms: 14400000\n      segment.bytes: 1073741824\n```\n\n**Kafka topics produced to:** reefers in case of anomaly detection or cold chain violation\n\n**Events reacted to:** telemetry events like:\n\n```java\npublic class TelemetryEvent {\n\n    public String containerID;\n    public Telemetry payload;\n    public String timestamp;\n    public String type;\n```\n\nand the payload:\n\n```java\n public String container_id;\n    public String measurement_time;\n    public String product_id;\n    public double temperature;\n    public double target_temperature; \n    public double ambiant_temperature; \n    public double kilowatts; \n    public double time_door_open;\n    public int content_type; \n    public int defrost_cycle;\n    public double oxygen_level; \n    public double nitrogen_level; \n    public double humidity_level;\n    public double target_humidity_level;\n    public double carbon_dioxide_level; \n    public boolean fan_1; \n    public boolean fan_2; \n    public boolean fan_3;\n    public double latitude;\n    public double longitude;\n  \n```\n**Events produced:** reefer anomaly detected and reefer cold chain violated\n\n### Code structure\n\nThe API is supported by the [ContainerResource](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent/blob/master/src/main/java/ibm/gse/eda/vaccine/coldchainagent/api/ContainerResource.java) class which exposes interactive query on the container id.\n\nThe core of the process is a Kafka Streams topology in the class [TelemetryAssessor](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent/blob/master/src/main/java/ibm/gse/eda/vaccine/coldchainagent/domain/TelemetryAssessor.java).\n\nThe [topology](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent/blob/b591e6e338cb8e9a5a8da5dde44d299f669d2309/src/main/java/ibm/gse/eda/vaccine/coldchainagent/domain/TelemetryAssessor.java#L86-L133) processes telemetry records and build a new streams with the containerID as key, and the telemetry payload as value. Then it builds a Ktable to keep aggregate per container. The aggregate is defined in [this ReeferAggregate class](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent/blob/master/src/main/java/ibm/gse/eda/vaccine/coldchainagent/domain/ReeferAggregate.java) and aims to keep max Temperature read so far, the number of time the maximum temperature is violated.\nFinally when a container reaches the maximum number of temperature violation, a new message is sent to a 'reefer' topic for down stream processing.\n\nIf you need to learn more on Kafka streams read [this introduction](https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-streams/) and do [those labs](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/kafka-streams/) to learn more on how to program with Kafka Streams.\n\nThe nice capability of Quarkus app, is most of the work is in the [application.properties](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent/blob/master/src/main/resources/application.properties) configuration. The highlights of this configuration is the fact that once deploy to OpenShift the environment variables are defined in config map and secret:\n\n```\nquarkus.openshift.env.configmaps=agent-cm\nquarkus.openshift.env.secrets=agent-secrets\n```\n\nThe SSL certificate for the server and the user are coming from secrets:\n\n```\nquarkus.openshift.env.vars.KAFKA_CERT_PATH=/deployments/certs/server/ca.p12\nquarkus.openshift.env.mapping.KAFKA_CERT_PWD.from-secret=kafka-cluster-ca-cert\nquarkus.openshift.env.mapping.KAFKA_CERT_PWD.with-key=ca.password\nquarkus.openshift.mounts.es-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.es-cert.secret-name=kafka-cluster-ca-cert\n# TLS user\nquarkus.openshift.env.mapping.USER_CERT_PWD.from-secret=${KAFKA_USER}\nquarkus.openshift.env.mapping.USER_CERT_PWD.with-key=user.password\nquarkus.openshift.env.vars.USER_CERT_PATH=/deployments/certs/user/user.p12\nquarkus.openshift.mounts.user.path=/deployments/certs/user\nquarkus.openshift.secret-volumes.user.secret-name=${KAFKA_USER}\n```\n\nThe outgoing message sent to Kafka `reefer` topic is done via microprofile reactive messaging configuration and plugin:\n\n```\nmp.messaging.outgoing.reefers.connector=smallrye-kafka\nmp.messaging.outgoing.reefers.topic=${REEFER_TOPIC:vaccine-reefers}\nmp.messaging.outgoing.reefers.key.serializer=org.apache.kafka.common.serialization.StringSerializer\nmp.messaging.outgoing.reefers.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n```\n\nThen the last part if for Kafka Streams.\n\n\n## Build\n\nAs a quarkus application it is possible to run locally with `./mvnw quarkus:dev`. It is important to configure a `.env` file with the needed environment variables to remote connect to Kafka Cluster using a SCRAM user:\n\n```\nexport KAFKA_USER=app-scram\nexport KAFKA_PASSWORD=<>\nexport KAFKA_BOOTSTRAP_SERVERS=eda-dev-kafka-bootstrap-eventstreams.<....>.cloud:443\nexport KAFKA_SSL_TRUSTSTORE_LOCATION=${PWD}/certs/truststore.p12\nexport KAFKA_SSL_TRUSTSTORE_PASSWORD=<>\nexport TELEMETRY_TOPIC=coldchain-telemetries\nexport REEFER_TOPIC=coldchain-reefers\nexport PREDICTION_ENABLED=false\nexport EDA_LOGGING_LEVEL=INFO\nexport KAFKA_SASL_MECHANISM=SCRAM-SHA-512\n```\n\nSee the [repository readme](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent) to build and run it locally or to deploy to openshift. \n\nAlso the [cold chain monitoring use case](/use-cases/cold-chain/) presents how to deploy on openshift, but it uses the Kubernetes config and source to image capability so one command will build and deploy to OpenShift:\n\n```shell\n./mvnw clean package -Dquarkus.kubernetes.deploy=true -DskipTests\n```\n\n## Usage details\n\nThe demonstration script for this component is described in the [cold chain monitoring use case](/use-cases/cold-chain/). ","fileAbsolutePath":"/home/runner/work/vaccine-solution-main/vaccine-solution-main/docs/src/pages/solution/cold-monitoring/index.mdx"}}}}